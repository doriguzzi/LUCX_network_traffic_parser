{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Network traffic classifier\n",
    "This notebook demonstrates how to train and evaluate two deep learning models, an MLP and a CNN, using datasets generated by LUCX. The key cells are located at the bottom of the notebook and enable both training and testing of the models.  \n",
    "The training phase is performed with randomized search, which tunes the main parameters of the deep learning models. The final two cells implement the testing phase: the first uses the test set produced by the parser, while the second allows the user to evaluate the model on live traffic, either captured directly from a network interface or obtained from a pre-recorded traffic trace in `pcap` format.\n",
    "\n",
    "## Python script\n",
    "This notebook can be converted into a Python script by running the following command:\n",
    "```jupyter nbconvert --to python NIDS.ipynb```.\n",
    "\n",
    "Once exported, the resulting `NIDS.py` script can be executed to train and test the model using the arguments defined in the second cell of the notebook.\n",
    "\n",
    "**Note**: Before exporting the notebook, modify the code in the last three cells by commenting out the line that uses the static paths for training or prediction, and uncommenting the line that uses the args parameters. For example, the current training cell is written as follows (to allow testing directly within the notebook):\n",
    "\n",
    "```train('./Dataset/DOS2019_Binary_5_Attacks_PCAPs','./nids_model.keras')```\n",
    "```#train(args.train,args.model)```\n",
    "\n",
    "Before converting the notebook into a Python script, it should be updated as:\n",
    "\n",
    "```#train('./Dataset/DOS2019_Binary_5_Attacks_PCAPs','./nids_model.keras')```\n",
    "```train(args.train,args.model)```\n",
    "\n",
    "Apply the same modification to the other two cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 @ FBK - Fondazione Bruno Kessler\n",
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: LUCX: LUCID network traffic parser eXtended\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pyshark\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.stats import uniform, randint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential,load_model, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Dropout, GlobalMaxPooling2D, Flatten\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from lucx_network_traffic_parser import *\n",
    "from util_functions import *\n",
    "\n",
    "# We need the following to get around “RuntimeError: This event loop is already running” when using Pyshark within Jupyter notebooks.\n",
    "# Not needed in stand-alone Python projects\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "EPOCHS = 1000\n",
    "TEST_ITERATIONS=10\n",
    "\n",
    "### SELECT THE MODEL_TYPE HERE ('MLP' or 'CNN') ###\n",
    "MODEL_TYPE = 'CNN' \n",
    "###################################################\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parser\n",
    "The following cell defines the arguments that are accepted by the `NIDS.py` Python script exported from this notebook. The arguments ```--train``` and ```--predict``` can be used to indicate the folder with the dataset. The script will load the ```hdf5``` files for training and testing respectively. The argument ```--predict_live``` can be used to perform predictions on live traffic captured from a network interface (e.g., ```eth0``` or ```lo```) or to make prediction using a pre-recorded traffic trace (e.g, ```ddos-chunk.pcap```). In both cases the argument is a string. In the first case, it is the name of the interface, in the second case, the path to the ```pcap``` file. The argument ```--model``` indicates the path to a trained model that will be used to make predictions (```--predict``` or ```predict_live```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"A DL-based NIDS for DDoS attack detection\")\n",
    "args = parser.parse_args(args=[])\n",
    "parser.add_argument('-t', '--train', nargs='?', type=str,  default=None, help=\"Start the training process\")\n",
    "parser.add_argument('-p', '--predict', nargs='?', type=str,  default=None, help=\"Perform a prediction on pre-preprocessed data\")\n",
    "parser.add_argument('-pl', '--predict_live', nargs='?', type=str, default=None, help='Perform a prediction on live traffic or on a pre-recorded traffic trace in pcap format')\n",
    "parser.add_argument('-m', '--model', type=str, default = None, help='File containing the model in keras format')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(\"see all args:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(Y_true, Y_pred, model_name, data_source, prediction_time):\n",
    "    ddos_rate = '{:04.3f}'.format(sum(Y_pred) / Y_pred.shape[0])\n",
    "\n",
    "    if Y_true is not None and len(Y_true.shape) > 0:  # if we have the labels, we can compute the classification accuracy\n",
    "        Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "        accuracy = accuracy_score(Y_true, Y_pred)\n",
    "\n",
    "        f1 = f1_score(Y_true, Y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0, 1]).ravel()\n",
    "        tnr = tn / (tn + fp)\n",
    "        fpr = fp / (fp + tn)\n",
    "        fnr = fn / (fn + tp)\n",
    "        tpr = tp / (tp + fn)\n",
    "\n",
    "        row = {'Model': model_name, 'Time': '{:04.3f}'.format(prediction_time),\n",
    "               'Samples': Y_pred.shape[0], 'DDOS%': ddos_rate, 'Accuracy': '{:05.4f}'.format(accuracy), 'F1Score': '{:05.4f}'.format(f1),\n",
    "               'TPR': '{:05.4f}'.format(tpr), 'FPR': '{:05.4f}'.format(fpr), 'TNR': '{:05.4f}'.format(tnr), 'FNR': '{:05.4f}'.format(fnr), 'Source': data_source}\n",
    "\n",
    "    pprint.pprint(row, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the DL models\n",
    "The following cell implements two deep learning models: an MLP and a CNN. Both can be trained using the traffic flow representations generated by LUCX. The MLP is trained on the flattened version of the data, whereas the CNN is trained on the array-like version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape,optimizer=Adam, dense_layers=4, hidden_units=2, learning_rate = 0.01,dropout_rate=0):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    for layer in range(dense_layers):\n",
    "        model.add(Dense(hidden_units, activation='relu', name='hidden-fc' + str(layer)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid', name='fc2'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape,optimizer=Adam, filters = 100, kernel_size=(3,3), strides=(1,1), padding='same',learning_rate = 0.01,dropout_rate=0.1):\n",
    "    model = Sequential(name  = \"cnn\")\n",
    "\n",
    "    model.add(Input(shape=(input_shape[1], input_shape[2], 1)))\n",
    "    model.add(Conv2D(filters=filters, kernel_size=kernel_size, data_format='channels_last', activation='relu', padding=padding, strides=strides))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid',name='output'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on static test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_path, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_test, y_test = load_dataset(dataset_path + \"/*\" + '-test.hdf5')\n",
    "\n",
    "        if model_path == None or model_path.endswith('.keras') == False:\n",
    "                print (\"No valid model specified!\")\n",
    "                return\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        pt0 = time.time()\n",
    "        for i in range(TEST_ITERATIONS):\n",
    "            Y_pred = np.squeeze(model.predict(X_test, batch_size=16) > 0.5,axis=1)\n",
    "        pt1 = time.time()\n",
    "        prediction_time = pt1 - pt0\n",
    "\n",
    "        report_results(np.squeeze(y_test), Y_pred,  model.name, '', prediction_time/TEST_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on live traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_live(source,model_path):\n",
    "    if source is not None:\n",
    "        if source.endswith('.pcap'):\n",
    "            pcap_file = source\n",
    "            cap = pyshark.FileCapture(pcap_file)\n",
    "            data_source = pcap_file.split('/')[-1].strip()\n",
    "        else:\n",
    "            cap =  pyshark.LiveCapture(interface=source)\n",
    "            data_source = args.predict_live\n",
    "\n",
    "        print (\"Prediction on network traffic from: \", source)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        # load the labels, if available\n",
    "        labels = parse_labels('DOS2019')\n",
    "\n",
    "        # Statistics on live traffic are computed considering benign vs malicious only\n",
    "        mc_labels = {'benign':0, 'malicious':1}\n",
    "\n",
    "\n",
    "        if MODEL_TYPE == 'MLP':\n",
    "            MAX_FLOW_LEN=10\n",
    "            mins, maxs = static_min_max(flatten=True,time_window=10,max_flow_len=MAX_FLOW_LEN)\n",
    "        else:\n",
    "            MAX_FLOW_LEN=10\n",
    "            mins, maxs = static_min_max(flatten=False,time_window=10,max_flow_len=MAX_FLOW_LEN)\n",
    "\n",
    "        while (True):\n",
    "            samples = process_live_traffic(cap, labels, mc_labels, max_flow_len=MAX_FLOW_LEN, traffic_type=\"all\",time_window=10)\n",
    "            if len(samples) > 0:\n",
    "                X,Y_true,flow_ids = dataset_to_list_of_fragments(samples)\n",
    "                if MODEL_TYPE == 'MLP':\n",
    "                    X = flatten_samples(X)\n",
    "                    X = np.array(normalize(X, mins, maxs))\n",
    "                else:\n",
    "                    X = np.array(normalize_and_padding(X, mins, maxs, MAX_FLOW_LEN))\n",
    "                if labels is not None:\n",
    "                    Y_true = np.array(Y_true)\n",
    "                else:\n",
    "                    Y_true = None\n",
    "                \n",
    "                pt0 = time.time()\n",
    "                Y_pred = np.squeeze(model.predict(X, batch_size=2048) > 0.5,axis=1)\n",
    "                pt1 = time.time()\n",
    "                prediction_time = pt1 - pt0\n",
    "\n",
    "                report_results(np.squeeze(Y_true), Y_pred,  MODEL_TYPE, '', prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Hyperparameter tuning with random search and k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_path, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_train, y_train = load_dataset(dataset_path + \"/*\" + '-train.hdf5')\n",
    "        X_val, y_val = load_dataset(dataset_path + \"/*\" + '-val.hdf5')\n",
    "\n",
    "        param_mlp_dist = {\n",
    "            'optimizer': [SGD, Adam],\n",
    "            'model__learning_rate': uniform(0.0001, 0.01),\n",
    "            'model__hidden_units': randint(8,32),\n",
    "            'model__dense_layers': randint(2,8)\n",
    "        }\n",
    "\n",
    "        param_cnn_dist = {\n",
    "            ### ADD YOUR CODE HERE ###\n",
    "            'model__learning_rate' : uniform(0.0001, 0.01),\n",
    "            'model__filters' : randint(16,64),\n",
    "            'optimizer' : [SGD,Adam],\n",
    "            'model__kernel_size': [(2,2),(3,3),(2,3)],\n",
    "            'model__strides': [(1,1),(2,2)],\n",
    "            'model__padding' : ['same', 'valid']\n",
    "            ##########################\n",
    "        }\n",
    "\n",
    "        if MODEL_TYPE == 'MLP':\n",
    "            param_dist = param_mlp_dist\n",
    "            create_model = create_mlp_model\n",
    "        else:\n",
    "            param_dist = param_cnn_dist\n",
    "            create_model = create_cnn_model\n",
    "\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "        model = KerasClassifier(model=create_model, input_shape=X_train.shape, batch_size=32, verbose=1,callbacks=[early_stopping])\n",
    "        random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=2, cv=2, random_state=SEED)\n",
    "        random_search_result = random_search.fit(X_train, y_train,epochs=1000, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "        # Print the best parameters and corresponding accuracy\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best cross-validated accuracy: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = random_search.best_estimator_.model_\n",
    "        if model_path is not None:\n",
    "            print (\"Model saved as: \" + model_path)\n",
    "            best_model.save(model_path)\n",
    "        else:\n",
    "            model_path = './nids_model-' + MODEL_TYPE + '.keras'\n",
    "            print (\"Model saved as: \" + model_path)\n",
    "            best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n",
    "Train the model by executing the method above with appropriate arguments (```args....``` see the ```argparse``` cell at the beginning of the notebook). This will prepare the code for the stand-alone script, which will accept arguments from the command line. \n",
    "If you want to train the model within this notebook for testing purposes, first call the ```train``` method with static dataset path (```'./DOS2019_Binary_5_Attacks_PCAPs'```) and model path (e.g., ```'./nids_model-MLP.keras'```). \n",
    "\n",
    "**NOTE**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous ```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_path = './nids_model-' + MODEL_TYPE + '.keras'\n",
    "train('./Dataset/DOS2019_Binary_5_Attacks_PCAPs','./nids_model.keras')\n",
    "#train(args.train,args.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "The following cell allows testing a model pre-trained model on the test set. \n",
    "If you want to test the model in this notebook, you can first call the ```predict``` method with static dataset path (```'./DOS2019_Binary_5_Attacks_PCAPs'```) and model path (e.g., ```'./nids_model-MLP.keras'```). \n",
    "\n",
    "**NOTE**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "\n",
    "model_path = './nids_model-' + MODEL_TYPE + '.keras'\n",
    "predict('./Dataset/DOS2019_Binary_5_Attacks_PCAPs',model_path)\n",
    "#predict(args.predict,args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using a pcap file\n",
    "The following cell allows testing a pre-trained model with live traffic by either collecting traffic from a network interface or by reading the packets from a pre-recorded traffic trace in `pcap` format. \n",
    "In the first case, call the ```predict_live``` method using the name of the interface (e.g., `eth0` or `lo`). In the second case, use the)path to a ```pcap``` file (e.g., ```'./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk.pcap'```). The `model_path` argument is the path to the model saved after training (e.g., ```'./nids_model-MLP.keras'```). \n",
    "\n",
    "**NOTE1**: these arguments are strings, do not forget the two paths between ```''```.\n",
    "**NOTE2**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on a pcap file\n",
    "\n",
    "model_path = './nids_model-' + MODEL_TYPE + '.keras'\n",
    "predict_live('./Dataset/DOS2019_Binary_5_Attacks_PCAPs_test/ddos-chunk.pcap',model_path)\n",
    "#predict_live(args.predict_live,args.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
