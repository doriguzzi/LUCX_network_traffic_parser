{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Network traffic classifier\n",
    "This notebook demonstrates how to train and evaluate two deep learning models, an MLP and a CNN, using datasets generated by LUCX. The key cells are located at the bottom of the notebook and enable both training and testing of the models.  \n",
    "The training phase is performed with randomized search, which tunes the main parameters of the deep learning models. The final two cells implement the testing phase: the first uses the test set produced by the parser, while the second allows the user to evaluate the model on live traffic, either captured directly from a network interface or obtained from a pre-recorded traffic trace in `pcap` format.\n",
    "\n",
    "## Python script\n",
    "This notebook can be converted into a Python script by running the following command:\n",
    "```jupyter nbconvert --to python NIDS.ipynb```.\n",
    "\n",
    "Once exported, the resulting `NIDS.py` script can be executed to train and test the model using the arguments defined in the second cell of the notebook. E.g.,\n",
    "\n",
    "```python NIDS.py --train sample_dataset/ --model_type mlp --model nids_model.keras```\n",
    "\n",
    "or\n",
    "\n",
    "```python NIDS.py --predict sample_dataset/ --model nids_model.keras --dataset_type DOS2019 --test_iterations 2```\n",
    "\n",
    "**Note**: Before exporting the notebook, modify the code in the last three cells by commenting out the line that uses the static paths for training or prediction, and uncommenting the line that uses the args parameters. For example, the current training cell is written as follows (to allow testing directly within the notebook):\n",
    "\n",
    "```train(dataset_path='./sample_dataset', model_type='mlp', model_path='./nids_model.keras')```\n",
    "\n",
    "```#train(dataset_path=args.train, model_type=args.model_type, model_path=args.model)```\n",
    "\n",
    "Before converting the notebook into a Python script, it should be updated as:\n",
    "\n",
    "```#train(dataset_path='./sample_dataset', model_type='mlp', model_path='./nids_model.keras')```\n",
    "\n",
    "```train(dataset_path=args.train, model_type=args.model_type, model_path=args.model)```\n",
    "\n",
    "Apply the same modification to the other two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 @ FBK - Fondazione Bruno Kessler\n",
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: LUCX: LUCID network traffic parser eXtended\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pyshark\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.stats import uniform, randint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential,load_model, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Dropout, GlobalMaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from lucx_network_traffic_parser import *\n",
    "from util_functions import *\n",
    "\n",
    "# We need the following to get around “RuntimeError: This event loop is already running” when using Pyshark within Jupyter notebooks.\n",
    "# Not needed in stand-alone Python projects\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "EPOCHS = 1000\n",
    "TEST_ITERATIONS=10\n",
    "MAX_FLOW_LEN = 10\n",
    "TIME_WINDOW = 10\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parser\n",
    "The following cell defines the arguments that are accepted by the `NIDS.py` Python script exported from this notebook. The arguments ```--train``` and ```--predict``` can be used to indicate the folder with the dataset. The script will load the ```hdf5``` files for training and testing respectively. The argument ```--predict_live``` can be used to perform predictions on live traffic captured from a network interface (e.g., ```eth0``` or ```lo```) or to make prediction using a pre-recorded traffic trace (e.g, ```ddos-chunk.pcap```). In both cases the argument is a string. In the first case, it is the name of the interface, in the second case, the path to the ```pcap``` file. The argument ```--model``` indicates the path to a trained model that will be used to make predictions (```--predict``` or ```predict_live```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"A DL-based NIDS for DDoS attack detection\")\n",
    "args = parser.parse_args(args=[])\n",
    "parser.add_argument('-t', '--train', nargs='?', type=str,  default=None, help=\"Start the training process\")\n",
    "parser.add_argument('-p', '--predict', nargs='?', type=str,  default=None, help=\"Perform a prediction on pre-preprocessed data\")\n",
    "parser.add_argument('-pl', '--predict_live', nargs='?', type=str, default=None, help='Perform a prediction on live traffic or on a pre-recorded traffic trace in pcap format')\n",
    "parser.add_argument('-m', '--model', type=str, default = 'nids_model.keras', help='File containing the model in keras format')\n",
    "parser.add_argument('-n', '--model_type', nargs='?', type=str,  default='mlp', help=\"Model type: MLP or CNN\")\n",
    "parser.add_argument('-d', '--dataset_type', type=str, default = 'DOS2019', help='Identifier of the dataset to be used (e.g., \"DOS2019\")')\n",
    "parser.add_argument('-i', '--test_iterations', type=int, default = TEST_ITERATIONS, help='Number of test iterations to be performed')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(\"see all args:\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the format of the labels\n",
    "The labels of the samples can be: (i) binary (0/1), (ii) multiclass one-hot encoded and (iii) multiclass integers. The following method is used before training and testing a model to tune the output layer, set the correct optimizer and to compare the prediction with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_info(y):    \n",
    "    # dimension of labels\n",
    "    output_dim = 1\n",
    "    labels = 'binary'\n",
    "    if y.ndim == 1: # binary classification or multiclass integer labels\n",
    "        if len(np.unique(y)) > 2 or max(y) > 1: # multiclass classification\n",
    "            output_dim=max(y)+1\n",
    "            labels = 'integer'\n",
    "    elif y.ndim == 2: # one-hot encoding multiclass\n",
    "        output_dim = y.shape[1]\n",
    "        labels = 'one-hot'\n",
    "\n",
    "    return output_dim, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_pred, model, samples, labels=None, data_source=None, prediction_time=None):\n",
    "\n",
    "    # --- Normalize predictions ---\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        # Multiclass: take argmax along classes axis (softmax)\n",
    "        if labels == 'BINARY':\n",
    "            # Special case: binary classification with one-hot encoding\n",
    "            y_pred_labels = np.minimum(1,np.argmax(y_pred, axis=1))\n",
    "        else:\n",
    "            y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        # Binary: threshold at 0.5\n",
    "        y_pred_labels = (y_pred.ravel() >= 0.5).astype(int)\n",
    "\n",
    "    # --- Normalize ground truth ---\n",
    "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "        # One-hot encoded ground truth\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "    else:\n",
    "        y_true_labels = y_true.ravel().astype(int)\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    acc = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    prec = precision_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true_labels, y_pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "    # --- Print results ---\n",
    "    print (\"----------------------------\")\n",
    "    print(f\"Model    : {model}\")\n",
    "    print(f\"Samples  : {samples}\")\n",
    "    if data_source is not None:\n",
    "        print(f\"Data Source.: {data_source}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"F1 Score : {f1:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    if prediction_time is not None:\n",
    "        print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "    print (\"----------------------------\")\n",
    "    if len(np.unique(y_true_labels)) <= 2 and max(y_true_labels) < 2: # binary classification\n",
    "        traffic_classes = DDOS_ATTACK_CLASSES['BINARY'] \n",
    "    else:    \n",
    "        traffic_classes = DDOS_ATTACK_CLASSES[labels] if labels in DDOS_ATTACK_CLASSES and labels is not None else range(len(np.unique(y_true_labels)))\n",
    "    \n",
    "    y_unique, counts = np.unique(y_pred_labels, return_counts=True)\n",
    "    for traffic_class in np.unique(y_true_labels):\n",
    "        print(\"{} samples: {}\".format(traffic_classes[traffic_class], sum(y_true_labels == traffic_class)))\n",
    "    print (\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the DL models\n",
    "The following cell implements two deep learning models: an MLP and a CNN. Both can be trained using the traffic flow representations generated by LUCX. The MLP is trained on the flattened version of the data, whereas the CNN is trained on the array-like version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape,output_shape=1, labels='binary', optimizer=Adam, dense_layers=4, hidden_units=2, learning_rate = 0.01,dropout_rate=0):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "\n",
    "    model.add(Input(shape=(input_shape[1],)))\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    for layer in range(dense_layers):\n",
    "        model.add(Dense(hidden_units, activation='relu', name='hidden-fc' + str(layer)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_shape, activation='softmax' if output_shape > 1 else 'sigmoid', name='fc2'))\n",
    "\n",
    "    if labels == 'one-hot':\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    elif labels == 'integer':\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:  # binary\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape,output_shape=1,labels='binary', optimizer=Adam, filters = 100, kernel_size=(3,3), strides=(1,1), padding='same',learning_rate = 0.01,dropout_rate=0.1):\n",
    "    model = Sequential(name  = \"cnn\")\n",
    "\n",
    "    model.add(Input(shape=(input_shape[1], input_shape[2], 1)))\n",
    "    model.add(Conv2D(filters=filters, kernel_size=kernel_size, data_format='channels_last', activation='relu', padding=padding, strides=strides))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(output_shape, activation='softmax' if output_shape > 1 else 'sigmoid', name='fc2'))\n",
    "\n",
    "    if labels == 'one-hot':\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    elif labels == 'integer':\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:  # binary\n",
    "        model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on static test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_path, model_path, dataset_type=None,test_iterations=TEST_ITERATIONS):\n",
    "    if dataset_path is not None:\n",
    "        X_test, y_test = load_dataset(dataset_path + \"/*\" + '-test.hdf5')\n",
    "\n",
    "        if model_path == None or model_path.endswith('.keras') == False:\n",
    "                print (\"No valid model specified!\")\n",
    "                return\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        pt0 = time.time()\n",
    "        for i in range(test_iterations):\n",
    "            y_pred = model.predict(X_test, batch_size=16,verbose=0)   \n",
    "        pt1 = time.time()\n",
    "        prediction_time = pt1 - pt0\n",
    "        evaluate_metrics(y_test, y_pred,model.name,X_test.shape[0],dataset_type, dataset_path,prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on live traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_live(source,model_path,dataset_type=None,test_iterations=TEST_ITERATIONS):\n",
    "    if source is not None:\n",
    "        if source.endswith('.pcap'):\n",
    "            pcap_file = source\n",
    "            cap = pyshark.FileCapture(pcap_file)\n",
    "            data_source = pcap_file.split('/')[-1].strip()\n",
    "        else:\n",
    "            cap =  pyshark.LiveCapture(interface=source)\n",
    "            data_source = args.predict_live\n",
    "\n",
    "        print (\"Prediction on network traffic from: \", source)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        # load the labels, if available\n",
    "        labels = parse_labels(dataset_type)\n",
    "\n",
    "        # Statistics on live traffic are computed considering benign vs malicious only\n",
    "        mc_labels = multiclass_labels('BINARY')\n",
    "\n",
    "\n",
    "        if model.name == 'mlp':\n",
    "            mins, maxs = static_min_max(flatten=True,time_window=TIME_WINDOW,max_flow_len=MAX_FLOW_LEN)\n",
    "        else:\n",
    "            mins, maxs = static_min_max(flatten=False,time_window=TIME_WINDOW,max_flow_len=MAX_FLOW_LEN)\n",
    "\n",
    "        iteration = 0\n",
    "        while (iteration < test_iterations):\n",
    "            samples = process_live_traffic(cap, labels, mc_labels, max_flow_len=MAX_FLOW_LEN, traffic_type=\"all\",time_window=TIME_WINDOW)\n",
    "            if len(samples) > 0:\n",
    "                X,y_true,flow_ids = dataset_to_list_of_fragments(samples)\n",
    "                if model.name == 'mlp':\n",
    "                    X = flatten_samples(X)\n",
    "                    X = np.array(normalize(X, mins, maxs))\n",
    "                else:\n",
    "                    X = np.array(normalize_and_padding(X, mins, maxs, MAX_FLOW_LEN))\n",
    "\n",
    "                if labels is not None:\n",
    "                    y_true = np.array(y_true)\n",
    "                else:\n",
    "                    y_true = None\n",
    "                \n",
    "                pt0 = time.time()\n",
    "                y_pred = model.predict(X, batch_size=16,verbose=0)\n",
    "                pt1 = time.time()\n",
    "                prediction_time = pt1 - pt0\n",
    "                iteration += 1\n",
    "                # hf = h5py.File('./live-prediction-test.hdf5', 'w')\n",
    "                # hf.create_dataset('set_x', data=X)\n",
    "                # hf.create_dataset('set_y', data=y_true)\n",
    "                # hf.create_dataset('set_y_pred', data=y_pred)\n",
    "                # hf.close()\n",
    "\n",
    "                print (\"Time window \", iteration)\n",
    "                evaluate_metrics(y_true, y_pred,model.name,X.shape[0],'BINARY', source, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Hyperparameter tuning with random search and k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_path, model_type, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_train, y_train = load_dataset(dataset_path + \"/*\" + '-train.hdf5')\n",
    "        X_val, y_val = load_dataset(dataset_path + \"/*\" + '-val.hdf5')\n",
    "\n",
    "        output_dim, labels = get_labels_info(y_train)\n",
    "\n",
    "        param_mlp_dist = {\n",
    "            'optimizer': [SGD, Adam],\n",
    "            'model__learning_rate': uniform(0.0001, 0.01),\n",
    "            'model__hidden_units': randint(4,8),\n",
    "            'model__dense_layers': randint(1,4)\n",
    "        }\n",
    "\n",
    "        param_cnn_dist = {\n",
    "            ### ADD YOUR CODE HERE ###\n",
    "            'model__learning_rate' : uniform(0.0001, 0.01),\n",
    "            'model__filters' : randint(16,64),\n",
    "            'optimizer' : [SGD,Adam],\n",
    "            'model__kernel_size': [(2,2),(3,3),(2,3)],\n",
    "            'model__strides': [(1,1),(2,2)],\n",
    "            'model__padding' : ['same', 'valid']\n",
    "            ##########################\n",
    "        }\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            param_dist = param_mlp_dist\n",
    "            create_model = create_mlp_model\n",
    "        else:\n",
    "            param_dist = param_cnn_dist\n",
    "            create_model = create_cnn_model\n",
    "\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loos', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "        model = KerasClassifier(model=create_model, input_shape=X_train.shape,  output_shape=output_dim, labels=labels, batch_size=32, verbose=1,callbacks=[early_stopping])\n",
    "        random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=2, cv=2, random_state=SEED)\n",
    "        random_search_result = random_search.fit(X_train, y_train,epochs=100, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "        # Print the best parameters and corresponding accuracy\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best cross-validated accuracy: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = random_search.best_estimator_.model_\n",
    "        if model_path is not None:\n",
    "            print (\"Model saved as: \" + model_path)\n",
    "            best_model.save(model_path)\n",
    "        else:\n",
    "            model_path = './nids_model-' + model_type + '.keras'\n",
    "            print (\"Model saved as: \" + model_path)\n",
    "            best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n",
    "Train the model by calling the `train` method above with the appropriate arguments: `dataset_path`, `model_type`, and `model_path`.\n",
    "\n",
    "The uncommented line in the cell below invokes `train` using the default values, allowing the command to run directly within the notebook. With the current defaults, the next cell will execute:\n",
    "\n",
    "```train(dataset_path='./sample_dataset', model_type='mlp', model_path='./nids_model.keras')```\n",
    "\n",
    "You can pass different arguments directly to the `predict` call below to specify alternative dataset paths, model types, or model locations.\n",
    "\n",
    "Before exporting the notebook to a Python script, remember to comment out the first line and uncomment the second one, which calls the `train` method using the arguments provided through the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "train(dataset_path='./sample_dataset', model_type='mlp', model_path='./nids_model.keras')\n",
    "#train(dataset_path=args.train, model_type=args.model_type, model_path=args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "Test a pre‑trained model by calling the `predict` method with the appropriate arguments: `dataset_path`, `model_path`, `dataset_type`, and `test_iterations`.\n",
    "\n",
    "The method call is already configured with default values so that the command can be executed directly within the notebook.\n",
    "\n",
    "With the current defaults, the next cell will run:\n",
    "\n",
    "```predict(dataset_path='./sample_dataset', model_path='./nids_model.keras', dataset_type='DOS2019', test_iterations=10)```\n",
    "\n",
    "You can pass different arguments directly to the `predict` call below to specify alternative dataset paths, model locations, dataset types and number of test iterations.\n",
    "\n",
    "Before exporting the notebook to a Python script, remember to comment out the first line and uncomment the second one, which calls the `predict` method using the arguments provided through the command line.\n",
    "\n",
    "**Note:**  \n",
    "- `dataset_type` is used to correctly compute the accuracy metrics.  \n",
    "- `test_iterations` specifies how many times the test should be repeated to compute the average processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "\n",
    "predict(dataset_path='./sample_dataset', model_path='./nids_model.keras', dataset_type='DOS2019', test_iterations=10)\n",
    "#predict(dataset_path=args.predict, model_path=args.model, dataset_type=args.dataset_type,test_iterations=args.test_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using a pcap file\n",
    "The following cell allows testing a pre-trained model with live traffic by either collecting traffic from a network interface or by reading the packets from a pre-recorded traffic trace in `pcap` format. \n",
    "\n",
    "Call the `predict_live` method with the appropriate arguments: `source`, `model_path`, `dataset_type`, and `test_iterations`.\n",
    "\n",
    "The method call is already  with default values so that the command can be executed directly within the notebook.\n",
    "\n",
    "With the current defaults, the next cell will run:\n",
    "\n",
    "```predict_live(source='./sample_dataset/DOS2019_Test_PCAPs/ddos-chunk.pcap', model_path='./nids_model.keras', dataset_type='DOS2019', test_iterations=10)```\n",
    "\n",
    "You can pass different arguments directly to the `predict_live` call below to specify alternative pcap path, network interface, model locations, dataset types and number of test iterations.\n",
    "\n",
    "Before exporting the notebook to a Python script, remember to comment out the first line and uncomment the second one, which calls the `predict` method using the arguments provided through the command line.\n",
    "\n",
    "**Note:**  \n",
    "- `source` is either the path to a pcap file or the name of a network interface (e.g., `eth0` or `lo`).\n",
    "- `dataset_type` is used to correctly compute the accuracy metrics.  \n",
    "- `test_iterations` specifies how many times the test should be repeated to compute the average processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on a pcap file\n",
    "\n",
    "predict_live(source='./sample_dataset/DOS2019_Test_PCAPs/ddos-chunk.pcap', model_path='./nids_model.keras', dataset_type='DOS2019', test_iterations=2)\n",
    "#predict_live(source=args.predict_live,model_path=args.model,dataset_type=args.dataset_type, test_iterations=args.test_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
